{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10318036,"sourceType":"datasetVersion","datasetId":6387979}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define features (X) and target (y)\nX = train.drop(columns=['target'])  # Replace 'target_column' with your actual target column name\ny = train['target']  # Replace 'target_column' with the actual name of your target column\n\n# Train-test split (80% training, 20% validation)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:44:00.385489Z","iopub.execute_input":"2025-01-01T17:44:00.385893Z","iopub.status.idle":"2025-01-01T17:44:00.427943Z","shell.execute_reply.started":"2025-01-01T17:44:00.385863Z","shell.execute_reply":"2025-01-01T17:44:00.426890Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Define the preprocessing pipeline\ndef create_refactored_preprocessing_pipeline():\n    # Columns to be encoded\n    categorical_columns = ['loan_type', 'New_versus_Repeat','lender_id']  # Update with actual categorical column names\n    \n    # Columns to be scaled\n    numerical_columns = ['Total_Amount', 'Total_Amount_to_Repay', 'Amount_Funded_By_Lender', \n                         'Lender_portion_Funded', 'Lender_portion_to_be_repaid']  # Update with actual numerical column names\n    \n    # Categorical transformer: handle missing values and one-hot encode\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent category\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical variables\n    ])\n    # Numerical transformer: handle missing values and scale\n    numerical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='mean')),            # Impute missing numerical values with the mean\n        ('scaler', MinMaxScaler())                            # Standardize numerical features\n    ])\n\n    # Combine transformers in a ColumnTransformer\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('cat', categorical_transformer, categorical_columns),\n            ('num', numerical_transformer, numerical_columns)\n        ])\n    \n    return preprocessor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:44:04.061891Z","iopub.execute_input":"2025-01-01T17:44:04.062237Z","iopub.status.idle":"2025-01-01T17:44:04.068050Z","shell.execute_reply.started":"2025-01-01T17:44:04.062209Z","shell.execute_reply":"2025-01-01T17:44:04.066980Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Create and fit the preprocessing pipeline\npreprocessor = create_refactored_preprocessing_pipeline()\n\n# Fit and transform the training data\nX_train_preprocessed = preprocessor.fit_transform(X_train)\nX_val_preprocessed = preprocessor.transform(X_val)  # Apply the same preprocessing to validation data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:44:08.660260Z","iopub.execute_input":"2025-01-01T17:44:08.660586Z","iopub.status.idle":"2025-01-01T17:44:08.818912Z","shell.execute_reply.started":"2025-01-01T17:44:08.660560Z","shell.execute_reply":"2025-01-01T17:44:08.817922Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Save the preprocessor\njoblib.dump(preprocessor, 'preprocessor3.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:44:12.597456Z","iopub.execute_input":"2025-01-01T17:44:12.598009Z","iopub.status.idle":"2025-01-01T17:44:12.607293Z","shell.execute_reply.started":"2025-01-01T17:44:12.597969Z","shell.execute_reply":"2025-01-01T17:44:12.606177Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['preprocessor3.pkl']"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Print the preprocessed train dataset\nprint(\"\\nPreprocessed Train Dataset:\")\nprint(X_train_preprocessed)\n\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical  \n\n# Convert target to categorical if it's a classification task\ny_train_encoded = to_categorical(y_train)\ny_val_encoded = to_categorical(y_val)           ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:44:17.694529Z","iopub.execute_input":"2025-01-01T17:44:17.694929Z","iopub.status.idle":"2025-01-01T17:44:27.527512Z","shell.execute_reply.started":"2025-01-01T17:44:17.694897Z","shell.execute_reply":"2025-01-01T17:44:27.526306Z"}},"outputs":[{"name":"stdout","text":"\nPreprocessed Train Dataset:\n  (0, 0)\t1.0\n  (0, 23)\t1.0\n  (0, 27)\t1.0\n  (0, 28)\t5.8956526865784944e-05\n  (0, 29)\t5.343301200078694e-05\n  (1, 0)\t1.0\n  (1, 23)\t1.0\n  (1, 27)\t1.0\n  (1, 28)\t5.208696105104009e-05\n  (1, 29)\t4.863269722604761e-05\n  (1, 30)\t0.00022500000000000002\n  (1, 31)\t0.25682311293822113\n  (1, 32)\t0.0002036964034133148\n  (2, 0)\t1.0\n  (2, 23)\t1.0\n  (2, 27)\t1.0\n  (2, 28)\t0.00023852175987145737\n  (2, 29)\t0.0002172339169781625\n  (3, 0)\t1.0\n  (3, 23)\t1.0\n  (3, 27)\t1.0\n  (3, 28)\t9.426087776181546e-05\n  (3, 29)\t8.538264804249459e-05\n  (3, 30)\t0.000406875\n  (3, 31)\t0.25682311293822113\n  :\t:\n  (54919, 32)\t0.00018722499612921928\n  (54920, 0)\t1.0\n  (54920, 23)\t1.0\n  (54920, 27)\t1.0\n  (54920, 28)\t0.0006828261463327083\n  (54920, 29)\t0.0006180208538264805\n  (54920, 30)\t0.0029450625000000006\n  (54920, 31)\t0.25682311293822113\n  (54920, 32)\t0.002587109037421939\n  (54921, 0)\t1.0\n  (54921, 23)\t1.0\n  (54921, 27)\t1.0\n  (54921, 28)\t0.00033834785550850916\n  (54921, 29)\t0.00031414518984851467\n  (54921, 30)\t0.0014595\n  (54921, 31)\t0.25682311293822113\n  (54921, 32)\t0.0013149673481802938\n  (54922, 0)\t1.0\n  (54922, 23)\t1.0\n  (54922, 27)\t1.0\n  (54922, 28)\t0.000708043539829873\n  (54922, 29)\t0.0006447767066692898\n  (54922, 30)\t0.0030538125000000005\n  (54922, 31)\t0.25682311293822113\n  (54922, 32)\t0.0026991146069537887\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Build the feed-forward neural network model\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(X_train_preprocessed.shape[1],)),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(y_train_encoded.shape[1], activation='softmax')  # Use softmax for multi-class classification\n])           # Compile the model\nmodel.compile(optimizer='adam', \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])            # Train the model\nhistory = model.fit(\n    X_train_preprocessed, y_train_encoded, \n    validation_data=(X_val_preprocessed, y_val_encoded),\n    epochs=20, \n    batch_size=32\n)          ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:44:57.225240Z","iopub.execute_input":"2025-01-01T17:44:57.225585Z","iopub.status.idle":"2025-01-01T17:46:12.437055Z","shell.execute_reply.started":"2025-01-01T17:44:57.225555Z","shell.execute_reply":"2025-01-01T17:46:12.436080Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9798 - loss: 0.1002 - val_accuracy: 0.9831 - val_loss: 0.0740\nEpoch 2/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9832 - loss: 0.0756 - val_accuracy: 0.9832 - val_loss: 0.0719\nEpoch 3/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9825 - loss: 0.0784 - val_accuracy: 0.9832 - val_loss: 0.0719\nEpoch 4/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9832 - loss: 0.0750 - val_accuracy: 0.9829 - val_loss: 0.0716\nEpoch 5/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9828 - loss: 0.0754 - val_accuracy: 0.9831 - val_loss: 0.0719\nEpoch 6/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9820 - loss: 0.0769 - val_accuracy: 0.9832 - val_loss: 0.0723\nEpoch 7/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9821 - loss: 0.0765 - val_accuracy: 0.9832 - val_loss: 0.0740\nEpoch 8/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9823 - loss: 0.0760 - val_accuracy: 0.9832 - val_loss: 0.0722\nEpoch 9/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9833 - loss: 0.0737 - val_accuracy: 0.9831 - val_loss: 0.0716\nEpoch 10/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9828 - loss: 0.0748 - val_accuracy: 0.9832 - val_loss: 0.0718\nEpoch 11/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9821 - loss: 0.0753 - val_accuracy: 0.9831 - val_loss: 0.0720\nEpoch 12/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9831 - loss: 0.0739 - val_accuracy: 0.9832 - val_loss: 0.0717\nEpoch 13/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9831 - loss: 0.0746 - val_accuracy: 0.9832 - val_loss: 0.0717\nEpoch 14/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9824 - loss: 0.0773 - val_accuracy: 0.9832 - val_loss: 0.0722\nEpoch 15/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9820 - loss: 0.0778 - val_accuracy: 0.9831 - val_loss: 0.0726\nEpoch 16/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9823 - loss: 0.0755 - val_accuracy: 0.9832 - val_loss: 0.0722\nEpoch 17/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9819 - loss: 0.0762 - val_accuracy: 0.9832 - val_loss: 0.0719\nEpoch 18/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9821 - loss: 0.0764 - val_accuracy: 0.9832 - val_loss: 0.0722\nEpoch 19/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9827 - loss: 0.0751 - val_accuracy: 0.9830 - val_loss: 0.0720\nEpoch 20/20\n\u001b[1m1717/1717\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9827 - loss: 0.0746 - val_accuracy: 0.9832 - val_loss: 0.0723\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Save the model after training\nmodel.save('fnn_model3.h5')\nprint(\"Model saved as 'fnn_model3.h5'\")\n\n# Load the saved model\nfrom tensorflow.keras.models import load_model\n\nmodel = load_model('fnn_model3.h5')\nprint(\"Model loaded successfully.\")\n\n# Evaluate using F1 Score on test data\ny_val_pred_probs = model.predict(X_val_preprocessed)\ny_val_pred = np.argmax(y_val_pred_probs, axis=1)\n\n# Calculate F1 Score\nf1 = f1_score(y_val, y_val_pred, average='macro')\nprint(f\"Test F1 Score: {f1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:46:18.701648Z","iopub.execute_input":"2025-01-01T17:46:18.701990Z","iopub.status.idle":"2025-01-01T17:46:19.910773Z","shell.execute_reply.started":"2025-01-01T17:46:18.701963Z","shell.execute_reply":"2025-01-01T17:46:19.909416Z"}},"outputs":[{"name":"stdout","text":"Model saved as 'fnn_model3.h5'\nModel loaded successfully.\n\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\nTest F1 Score: 0.5936704228947385\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Load the saved model\nfrom tensorflow.keras.models import load_model\n\nmodel = load_model('/kaggle/working/fnn_model3.h5')\nprint(\"Model loaded successfully.\")\n\nimport joblib\n# Load the preprocessor from the saved file\npreprocessor = joblib.load('/kaggle/working/preprocessor3.pkl')  \n\nimport pandas as pd\n# Load your test data from test.csv\ntest_data = pd.read_csv('/kaggle/input/zindidataset/Test.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:47:00.713731Z","iopub.execute_input":"2025-01-01T17:47:00.714065Z","iopub.status.idle":"2025-01-01T17:47:00.831041Z","shell.execute_reply.started":"2025-01-01T17:47:00.714040Z","shell.execute_reply":"2025-01-01T17:47:00.829982Z"}},"outputs":[{"name":"stdout","text":"Model loaded successfully.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Apply the preprocessor to the test data\nX_test = test_data  \nX_test_preprocessed = preprocessor.transform(X_test) \n\n# Make predictions on the test data\nimport numpy as np \ny_test_pred_probs = model.predict(X_test_preprocessed)  # Get prediction probabilities\ny_test_pred = np.argmax(y_test_pred_probs, axis=1)  # Get the class with highest probability (predicted class)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:47:04.723659Z","iopub.execute_input":"2025-01-01T17:47:04.724060Z","iopub.status.idle":"2025-01-01T17:47:06.161764Z","shell.execute_reply.started":"2025-01-01T17:47:04.724029Z","shell.execute_reply":"2025-01-01T17:47:06.160928Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Prepare the submission DataFrame\nsubmission = pd.DataFrame({\n    'ID': test_data['ID'],  # Replace 'ID' with the correct column name from your test dataset\n    'Target': y_test_pred  # The predicted target values (0 or 1)\n})\n\n# Save the submission file\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"Submission file 'submission.csv' has been saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:47:10.777224Z","iopub.execute_input":"2025-01-01T17:47:10.777552Z","iopub.status.idle":"2025-01-01T17:47:10.807878Z","shell.execute_reply.started":"2025-01-01T17:47:10.777527Z","shell.execute_reply":"2025-01-01T17:47:10.806956Z"}},"outputs":[{"name":"stdout","text":"Submission file 'submission.csv' has been saved.\n","output_type":"stream"}],"execution_count":13}]}